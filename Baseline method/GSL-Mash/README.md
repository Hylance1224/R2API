# GSL-Mash

My code is built on SMRB <https://github.com/ssnowyu/SMRB>
Dataet also be seen there

# Project Structure

The directory structure of new project looks like this:

    â”‚   README.md
    â”‚   requirements.txt               <- File for installing python dependencies
    â”‚   setup.cfg                      <- Configuration of linters and pytest
    â”‚   test.py                        <- Run testing
    â”‚   train.py                       <- Run training
    â”‚
    â”œâ”€â”€â”€configs                        <- Hydra configuration files
    â”‚   â”‚   test.yaml                     <- Main config for testing
    â”‚   â”‚   train.yaml                    <- Main config for training
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€callbacks                  <- Lightning callbacks
    â”‚   â”‚       wandb.yaml                <- Wandb and metrics callbacks
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€datamodule                    
    â”‚   â”‚       partial_text_bert.yaml    <- Partial text-based dataset embedded by BERT configs
    â”‚   â”‚       partial_word_bert.yaml    <- Partial word-based dataset embedded by BERT configs
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€experiment                 <- Experiment configs
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€hparams_search             <- Hyperparameter search configs
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€logger                     <- Logger configs
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€log_dir                    <- Logging directory configs
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€model                      <- Model configs
    â”‚   â”‚
    â”‚   â””â”€â”€â”€trainer                    <- Trainer configs
    â”‚
    â”œâ”€â”€â”€data                        <- Project data
    â”‚
    â”œâ”€â”€â”€logs                        <- Logs generated by Hydra and PyTorch 
                                       Lightning loggers
    â”œâ”€â”€â”€src                         <- Source code
    â”‚   â”‚   testing_pipeline.py
    â”‚   â”‚   training_pipeline.py
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€callbacks
    â”‚   â”‚       wandb_callbacks.py
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€datamodules             <- Lightning datamodules
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€models                  <- Lightning models
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€utils                   <- Utility scripts
    â”‚   â”‚
    â”‚   â””â”€â”€â”€vendor                  <- Third party code that cannot be installed using PIP/Conda
    â”‚
    â””â”€â”€â”€tests                       <- Tests of any kind
        â”‚
        â”œâ”€â”€â”€helpers                    <- A couple of testing utilities
        â”‚
        â”œâ”€â”€â”€shell                      <- Shell/command based tests
        â”‚
        â””â”€â”€â”€unit                       <- Unit tests

# Installation

You can install environment by anaconda, and then download the dataset.

## Install with anaconda

    # clone project
    git clone https://github.com/MyToumaKazusa/GSL-Mash
    cd GSL-Mash

## Create conda environment

    conda create -n gsl-mash python=3.10
    conda activate gsl-mash

## Install requirements

    pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116
    conda install pip==24.0
    pip install -r requirements.txt
    pip install torchmetrics==0.9.1
    wget https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_cluster-1.6.0%2Bpt113cu116-cp310-cp310-linux_x86_64.whl
    wget https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.15%2Bpt113cu116-cp310-cp310-linux_x86_64.whl
    wget https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp310-cp310-linux_x86_64.whl
    # Remember to install the wheels built by wget
    # If you are not using Linux, search this website for your packages
    # https://pytorch-geometric.com/whl/torch-1.13.1%2Bcu116.html

# Quickstart

Implement the DL-based model as the LightningModule class. Details refer to Model Implementation. Here the GSL-Mash model (pre-configured in our project) is used as an example.

Write a configuration file called simple\_model for your model.

```
 _target_: src.models.GSL-Mash.GSL_MASH

data_dir: ${data_dir}/api_mashup
api_embed_path: embeddings/partial/text_bert_api_embeddings.npy
mashup_embed_channels: 300
mlp_output_channels: 300
weight_decay: 0.00001
 
```

Write a configuration file called mlp for your experiment.

    _target_: src.datamodules.mashup_datamodule_new.MashupDataModule

    data_dir: ${data_dir}/api_mashup
    num_candidates: 932
    mashup_path: embeddings/text_bert_mashup_embeddings.npy
    api_path: embeddings/partial/text_bert_api_embeddings.npy
    invoked_path: partial_invocation.pkl
    pair_in_training: true
    negative_samples_ratio: 5

    train_val_test_split: [3190, 912, 455] # 7: 2: 1
    batch_size: 64 #SMRBgsl 16
    num_workers: 12
    pin_memory: true 

Since the project uses wandb as the log framework by default, you will need to have a wandb account and bind the account to the project by executing the following command.

    wandb login

This command needs to be executed only once during the entire development process.

If you do not want to use wandb, you can also choose another log framework. Please refer to LightningModule for how to change it.

run the project.

    python train.py experiment=gsl-mash/partial_bert

## **Supplementary Instructions (added by Yihui Wang**ðŸ‘©â€ðŸ’»)

The following section provides additional guidance for generating dataset for the project.

To generate the dataset, start Python in **command line** mode and run:

    python generate_dataset.py

